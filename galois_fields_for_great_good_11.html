<!DOCTYPE html>
<html lang="en">
<head>
  <link rel="stylesheet" href="style.css" />
  <link rel="icon" type="image/x-icon" href="/favicon.ico">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css">

  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>

  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"></script>

  <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          // customised options
          // • auto-render specific keys, e.g.:
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
          ],
          // • rendering keys, e.g.:
          throwOnError : false
        });
    });
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>xorvoid</title>
</head>

<body class="grayscale">
  <div id="header">
    <h1><a href="/">xorvoid</a></h1>
    <p>
      <a href="/">home</a>
      <a href="https://github.com/xorvoid">github</a>
      <a href="rss.xml">rss</a>
      <a href="https://www.linkedin.com/in/anthony-bonkoski-2563a158">linkedin</a>
    </p>
    <p>
      <a href="https://www.buymeacoffee.com/xorvoid">buy me a coffee</a>
    </p>
    <hr>
  </div>
  <div id="main">
<h1>Learn you Galois Fields for Great Good (11)</h1>
<p>Navigation |
<a href="galois_fields_for_great_good_00.html">first</a> |
<a href="galois_fields_for_great_good_10.html">previous</a> |
next (soon)</p>
<!--
[next](galois_fields_for_great_good_12.html)
-->

<h2>Reed-Solomon as Linear Algebra</h2>
<p>This is part 11 of a series on Abstract Algebra. In this part, we'll continue exploring Reed-Solomon Erasure Codes.</p>
<p>Last time we introduced Reed-Solomon as polynomial evaluation (encoding) and polynomial interpolation (decoding). This time, we'll recast those ideas in the framework of linear algebra. This will give us new capabilities and variants.</p>
<p><strong>Background required:</strong> This article assumes the reader has a good understanding of linear algebra, including the concepts: matrix multiplication, inverses, solving linear-systems with LU factorization, linear independence, linear combinations, matrix rank, non-singularity conditions, etc.</p>
<p>If you are missing some background, checkout the resources listed in <a href="galois_fields_for_great_good_09.html">part 9</a>.</p>
<h3>A Matrix for Polynomial Evaluation</h3>
<p>Let's continue with the example from the previous article using the data vector <code>[3, 2, 5]</code> representing the polynomial <code>p(x) = 3 + 2x + 5x^2</code>. Suppose we wish to evaluate this polynomial at <code>x = 2</code>.</p>
<p>Using linear algebra, we can represent this evaluation as an <em>inner-product</em> (also called a <em>dot-product</em>):</p>
<p>
$$
\begin{align}
p(2) &=
3 + 2(2) + 5(2)^2
\nonumber
\\
&= \left[ \begin{array}{ccc}
1 & 2 & 2^2 \\
\end{array} \right]

\left[ \begin{array}{ccc}
3 \\ 2 \\ 5 \\
\end{array} \right]
\nonumber \\
&= 27
\nonumber
\end{align}
$$
</p>

<p>Similarly, we can evaluate many points $\textbf{x} = (1, 2, 3, 4, 5)$ with a single matrix-vector product:</p>
<p>
$$
\begin{align}
p(\textbf{x})
&= \left[ \begin{array}{ccc}
1 & 1 & 1^2 \\
1 & 2 & 2^2 \\
1 & 3 & 3^2 \\
1 & 4 & 4^2 \\
1 & 5 & 5^2 \\
\end{array} \right]

\left[ \begin{array}{ccc}
3 \\ 2 \\ 5 \\
\end{array} \right]
\nonumber

= \left[ \begin{array}{ccc}
10 \\
27 \\
54 \\
91 \\
138 \\
\end{array} \right]
\nonumber \\

&= \textbf{A}
\left[ \begin{array}{ccc}
3 \\ 2 \\ 5 \\
\end{array} \right]
\nonumber
\end{align}
$$
</p>

<h3>Vandermonde Matrices</h3>
<p>The $\textbf{A}$ matrix constructed above is known as a <a href="https://en.wikipedia.org/wiki/Vandermonde_matrix"><em>Vandermonde matrix</em></a>.</p>
<p>Given some distinct constants $x_0, x_1, \dots, x_{n-1}$ the vandermonde matrix has the form:</p>
<p>
$$
\left[ \begin{array}{ccc}
1 & x_0 & x_0^2 & ... & x_0^{n-1} \\
1 & x_1 & x_1^2 & ... & x_1^{n-1} \\
1 & x_2 & x_2^2 & ... & x_2^{n-1} \\
1 & x_3 & x_3^2 & ... & x_3^{n-1} \\
1 & x_4 & x_4^2 & ... & x_4^{n-1} \\
\vdots & & & & \vdots \\
1 & x_{n-1} & x_{n-1}^2 & ... & x_{n-1}^{n-1} \\
\end{array} \right]
$$
</p>

<p>A square $n \times n$ vandermonde matrix is always invertible (as long as all $x_i$ are distinct).</p>
<p>This gives intuition to what's happening in Reed-Solomon. Suppose we construct a $5 \times 3$ Vandermonde matrix (as above), then we can <strong>erase</strong> any 2 rows and obtain a square $3 \times 3$ Vandermonde matrix that is invertible!</p>
<p>This property suggests a very straightforward approach to implementing Reed-Solomon using ordinary linear algebra.</p>
<h3>Encoding/Decoding in Matrix Form</h3>
<p>A few definitions:</p>
<ul>
<li>$\textbf{A}$: Vandermonde matrix for our desired Reed-Solomon code</li>
<li>$\textbf{d}$: Vector of unencoded data</li>
<li>$\textbf{e}$: Vector of encoded data</li>
</ul>
<p>Encoding is:</p>
<p>
$$
\textbf{e} = \textbf{A} \textbf{d}
$$
</p>

<p>Decoding is:</p>
<ol>
<li>Erase rows from $\textbf{A}$ and $\textbf{e}$ until we have a square Vandermonde $\textbf{A}_r$ and the corresponding $\textbf{e}_r$</li>
<li>Solve the linear system for $d$:</li>
</ol>
<p>
$$
\textbf{A}_r \textbf{d} = \textbf{e}_r
$$
</p>

<p>It's exactly that simple!</p>
<h3>A Worked Example</h3>
<p>Let's work through an example to drive the idea home. We'll use the $5 \times 3$ Vandermonde matrix we constructed above:</p>
<p>
$$
\textbf{A} = 
\left[ \begin{array}{ccc}
1 & 1 & 1^2 \\
1 & 2 & 2^2 \\
1 & 3 & 3^2 \\
1 & 4 & 4^2 \\
1 & 5 & 5^2 \\
\end{array} \right]
=
\left[ \begin{array}{ccc}
1 & 1 & 1 \\
1 & 2 & 4 \\
1 & 3 & 9 \\
1 & 4 & 16 \\
1 & 5 & 25 \\
\end{array} \right]
$$
</p>

<p>But, for fun, let's use a new data vector:</p>
<p>
$$
\textbf{d} = 
\left[ \begin{array}{ccc}
6 \\ 0 \\ 2
\end{array} \right]
$$
</p>

<p>Let's encode:</p>
<p>
$$
\textbf{e} =
\textbf{A} \textbf{d} = 
\left[ \begin{array}{ccc}
1 & 1 & 1 \\
1 & 2 & 4 \\
1 & 3 & 9 \\
1 & 4 & 16 \\
1 & 5 & 25 \\
\end{array} \right]
\left[ \begin{array}{ccc}
6 \\ 0 \\ 2
\end{array} \right]
=
\left[ \begin{array}{ccc}
8 \\ 14 \\ 24 \\ 38 \\ 56 \\
\end{array} \right]
$$
</p>

<p>Easy peasy.</p>
<p>Okay, let's decode it. We'll erase the first two rows to get a square $3 \times 3$ matrix:</p>
<p>
$$
\textbf{A}_r = 
\left[ \begin{array}{ccc}
1 & 3 & 9 \\
1 & 4 & 16 \\
1 & 5 & 25 \\
\end{array} \right]\\
\textbf{e}_r
=
\left[ \begin{array}{ccc}
24 \\ 38 \\ 56 \\
\end{array} \right]
$$
</p>

<p>Let's solve the system $\textbf{A}_r \textbf{d} = \textbf{e}_r$ using an $\textbf{A}_r = \textbf{L}\textbf{U}$ factorization:</p>
<p>
$$
\left[ \begin{array}{ccc}
1 & 3 & 9 \\
1 & 4 & 16 \\
1 & 5 & 25 \\
\end{array} \right]
=
\left[ \begin{array}{ccc}
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 2 & 1 \\
\end{array} \right]
\left[ \begin{array}{ccc}
1 & 3 & 9 \\
0 & 1 & 7 \\
0 & 0 & 2 \\
\end{array} \right]\\
$$
</p>

<p>Solving $\textbf{L}\textbf{y} = \textbf{e}_r$</p>
<p>
$$
\left[ \begin{array}{ccc}
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 2 & 1 \\
\end{array} \right]
\textbf{y}
=
\left[ \begin{array}{ccc}
24 \\ 38 \\ 56 \\
\end{array} \right]
$$
$$
\textbf{y}
=
\left[ \begin{array}{ccc}
24 \\ 14 \\ 4 \\
\end{array} \right] \\
$$
</p>

<p>Solving $\textbf{U}\textbf{d} = \textbf{y}$</p>
<p>
$$
\left[ \begin{array}{ccc}
1 & 3 & 9 \\
0 & 1 & 7 \\
0 & 0 & 2 \\
\end{array} \right]
\textbf{d} =
\left[ \begin{array}{ccc}
24 \\ 14 \\ 4 \\
\end{array} \right] \\
$$
$$
\textbf{d}
=
\left[ \begin{array}{ccc}
6 \\ 0 \\ 2 \\
\end{array} \right] \\
$$
</p>

<p>As desired $\square$</p>
<p><i><u>Exercise:</u></i> Repeat this example, but erase the last two rows. You should still recover the $d$ vector</p>
<h3>Moving beyond Vandermonde</h3>
<p>Above we just used ordinary linear algebra to do the same operations as polynomial evaluation and polynomial interpolation without a single polynomial in sight! The reason we can do this all comes back the invertibility of Vandermonde matrices.</p>
<p>But this leads to a new ideas. Why does it have to be Vandermonde? Are there other matrices with the same invertibility properties? Can we use our linear algebra tools to construct one?</p>
<p>What kind of matrix would be useful?</p>
<h3>Systematic encoding matrices</h3>
<p>One very useful $3 \times 5$ encoding matrix looks like:</p>
<p>
$$
\left[ \begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
a & b & c \\
d & e & f \\
\end{array} \right]
$$
</p>

<p>This is called a <em>systematic encoding matrix</em> because the first <code>k</code> encoded elements are the same as the unencoded data:</p>
<p>
$$
\left[ \begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
a & b & c \\
d & e & f \\
\end{array} \right]
\left[ \begin{array}{ccc}
6 \\ 7 \\ 8 \\
\end{array} \right]
=
\left[ \begin{array}{ccc}
6 \\ 7 \\ 8 \\ p \\ q \\
\end{array} \right]
$$
</p>

<p>The additional rows <code>p</code> and <code>q</code> are often commonly called <em>parity</em> (for historical reasons).</p>
<p>On encode, we only have to compute the parity rows (<code>p</code> and <code>q</code>). And, our decode is a trivial identity function unless one of the original data elements is erased. These features make systematic codes very popular in data-storage systems where storage devices fail infrequently.</p>
<h3>Constructing systematic matrices</h3>
<p>There are many ways to construct such a systematic matrix. We'll demonstrate a method that starts with a Vandermonde matrix and transforms it into a systematic matrix.</p>
<p>Recall our $5 \times 3$ vandermonde matrix:</p>
<p>
$$
\textbf{A} = 
\left[ \begin{array}{ccc}
1 & 1 & 1 \\
1 & 2 & 4 \\
1 & 3 & 9 \\
1 & 4 & 16 \\
1 & 5 & 25 \\
\end{array} \right]
$$
</p>

<p>Let's invert the upper $3 \times 3$ square:</p>
<p>
$$
\textbf{A}_r^{-1} = 
\left( \left[ \begin{array}{ccc}
1 & 1 & 1 \\
1 & 2 & 4 \\
1 & 3 & 9 \\
\end{array} \right] \right) ^ {-1}
=
\left[ \begin{array}{ccc}
 3   &  -3  &   1   \\
-2.5 &   4  &  -1.5 \\
 0.5 &  -1  &   0.5 \\
\end{array} \right]
$$
</p>

<p>Now we can multiply with $\textbf{A}$:</p>
<p>
$$
\begin{align}
\textbf{A} \textbf{A}_r^{-1}
&=
\left[ \begin{array}{ccc}
1 & 1 & 1 \\
1 & 2 & 4 \\
1 & 3 & 9 \\
1 & 4 & 16 \\
1 & 5 & 25 \\
\end{array} \right]
\left[ \begin{array}{ccc}
 3   &  -3  &   1   \\
-2.5 &   4  &  -1.5 \\
 0.5 &  -1  &   0.5 \\
\end{array} \right]
\nonumber \\
&= 
\left[ \begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & -3 & 3 \\
3 & -8 & 6 \\
\end{array} \right]
\nonumber
\end{align}
$$
</p>

<p>And now we've obtained the desired "parity" encoding rows:</p>
<p>
$$
\left[ \begin{array}{ccc}
1 & -3 & 3 \\
3 & -8 & 6 \\
\end{array} \right]
$$
</p>

<h2>Implementing Reed-Solomon</h2>
<p>Let's implement all of this in code! Like the previous section, we'll implement over <code>GF(256)</code>. And we'll use the linear algebra routines developed in <a href="galois_fields_for_great_good_09.html">part 9</a>.</p>
<h3>Source Code: <a href="https://github.com/xorvoid/learn_you_galois_fields_for_great_good/blob/main/src/reed_solomon_linalg.rs"><code>src/reed_solomon_linalg.rs</code></a></h3>
<pre><code class="language-rust">#![allow(non_snake_case)]
#![allow(dead_code)]
use crate::gf_256::GF;
use crate::linalg::Matrix;
</code></pre>
<p>As before, we need some parameters:</p>
<pre><code class="language-rust">const K: usize = 3;  // Number of data elements (before encoding)
const N: usize = 5;  // Number of encoded elements
</code></pre>
<p>We also need to specify the evaluation points. We need to have <code>N</code> of these.
For this variant of Reed-Solomon, it really doesn't matter what we pick. They just need to be
distinct elements of <code>GF(256)</code>.</p>
<pre><code class="language-rust">const X: [GF; 5] = [
    GF::new(42), GF::new(222), GF::new(2), GF::new(8), GF::new(99)
];
</code></pre>
<h4>Encoding and Decoding with a Matrix</h4>
<p>For encoding, we'll use a generic matrix <code>A</code>. This matrix should have the required
invertibility properties.</p>
<p>We just need to perform a simple multiply (<code>e = A*d</code>):</p>
<pre><code class="language-rust">pub fn reed_solomon_linalg_encode(A: &amp;Matrix, data: &amp;[GF]) -&gt; Vec&lt;GF&gt; {
    // Sanity check
    assert_eq!(data.len(), K);

    // Construct the encoding matrix (A) and data vector (d)
    let d = Matrix::new(K, 1, data.to_vec());

    // Encode: e = A*d
    let e = A.matmul(&amp;d);

    // Return the raw data
    e.data().to_vec()
}
</code></pre>
<p>Decoding will use the same <code>A</code> matrix, removing any rows for erased elements.</p>
<p>We just need to solve a linear system (solve <code>A_r * d = e_r</code> for <code>d</code>):</p>
<pre><code class="language-rust">pub fn reed_solomon_linalg_decode(A: &amp;Matrix, encoded: &amp;[Option&lt;GF&gt;]) -&gt; Option&lt;Vec&lt;GF&gt;&gt; {
    // Sanity check
    assert_eq!(encoded.len(), N);

    // First, we need to gather up evaluations that haven't been erased.
    let mut evals = vec![];
    let mut idx = vec![];
    for i in 0..N {
        if let Some(value) = encoded[i] {
            evals.push(value);
            idx.push(i);
        }
    }

    // Make sure we have enough evaluations to decode
    if evals.len() &lt; K {
        return None; // Too many erasures, can't decode!
    }

    // Select the rows from the
    // Construct the matrix (A_r)
    let A_r = A.select_rows(&amp;idx[..K]);
    let e_r = Matrix::new(K, 1, evals[..K].to_vec());

    // Decode by linear solve: A_r * d = e_r
    let d = A_r.solve(&amp;e_r).unwrap();

    // Return the raw data
    Some(d.data().to_vec())
}
</code></pre>
<h4>Vandermonde Matrices</h4>
<p>We'll build a Vandermonde Matrix out of <code>N</code> evaluation points (<code>xs</code>)</p>
<pre><code class="language-rust">pub fn vandermonde_encoding_matrix(xs: &amp;[GF]) -&gt; Matrix {
    // Sanity check
    assert_eq!(xs.len(), N);

    // The resulting matrix will be an N x K Vandermonde
    let mut mat = Matrix::zeros(N, K);

    // Build up one row at a time
    for i in 0..N {
        // Compute each value in the row: 1, x, x^2, x^3, etc
        let mut value = GF::new(1);
        for j in 0..K {
            mat[(i,j)] = value;
            value = value * xs[i];
        }
    }

    mat
}
</code></pre>
<h4>Testing Time</h4>
<p>Just like the polynomial version, we should be able to decode up to 2-element erasures.
But, 3-element erasures will fail.</p>
<p>Let's test each possibility.</p>
<pre><code class="language-rust">#[cfg(test)]
fn encode_decode_all(A: &amp;Matrix, data: &amp;[GF], expected_enc: &amp;[GF]) {

    // encode
    let enc = reed_solomon_linalg_encode(A, data);
    assert_eq!(enc, expected_enc);
    let recv_all: Vec&lt;_&gt; = enc.iter().map(|x| Some(*x)).collect();

    // decode with no erasures: success!
    assert_eq!(reed_solomon_linalg_decode(A, &amp;recv_all), Some(data.to_vec()));

    // decode with all one element erasures: success!
    for i in 0..N {
        let mut recv = recv_all.clone();
        recv[i] = None;
        assert_eq!(reed_solomon_linalg_decode(A, &amp;recv), Some(data.to_vec()));
    }

    // decode with all two element erasures: success!
    for i in 0..N {
        for j in (i+1)..N {
            let mut recv = recv_all.clone();
            recv[i] = None;
            recv[j] = None;
            assert_eq!(reed_solomon_linalg_decode(A, &amp;recv), Some(data.to_vec()));
        }
    }

    // decode with all three element erasures: failure!
    for i in 0..N {
        for j in (i+1)..N {
            for k in (j+1)..N {
                let mut recv = recv_all.clone();
                recv[i] = None;
                recv[j] = None;
                recv[k] = None;
                assert_eq!(reed_solomon_linalg_decode(A, &amp;recv), None);
            }
        }
    }
}
</code></pre>
<p>Now we can test a bunch of different data vectors. Note: these are the exact same ones
as in <code>reed_solomon_poly.rs</code>, giving an empirical example of equivalence.</p>
<pre><code class="language-rust">#[cfg(test)]
#[test]
fn test_vandermonde_encode_decode() {
    // construct our vandermonde encoding matrix
    let A = vandermonde_encoding_matrix(&amp;X);

    // test: trivial
    encode_decode_all(&amp;A,
        &amp;[GF::new(0), GF::new(0), GF::new(0)],
        &amp;[GF::new(0), GF::new(0), GF::new(0), GF::new(0), GF::new(0)],
    );

    // test: ones
    encode_decode_all(&amp;A,
        &amp;[GF::new(1), GF::new(1), GF::new(1)],
        &amp;[GF::new(3), GF::new(161), GF::new(7), GF::new(73), GF::new(160)],
    );

    // test: pattern
    encode_decode_all(&amp;A,
        &amp;[GF::new(100), GF::new(150), GF::new(200)],
        &amp;[GF::new(160), GF::new(135), GF::new(94), GF::new(104), GF::new(194)],
    );

    // test: random
    encode_decode_all(&amp;A,
        &amp;[GF::new(216), GF::new(196), GF::new(171)],
        &amp;[GF::new(81), GF::new(157), GF::new(209), GF::new(193), GF::new(105)],
    );
}
</code></pre>
<h4>Systematic Matrices</h4>
<p>We will now construct systematic matrices by transforming an existing <code>A</code> matrix
using the <code>A * inv(A_r)</code> approach:</p>
<pre><code class="language-rust">pub fn systematic_encoding_matrix(A: &amp;Matrix) -&gt; Matrix {
    // Compute the inverse of the upper K x K square
    let inv = A.slice_rows(0..K).inverse().unwrap();

    // Multiply it
    A.matmul(&amp;inv)
}
</code></pre>
<h4>Testing again</h4>
<p>We can completely reuse the encoding and decoding routines for the systematic matrices.
This is a very inefficient approach. In practice, you'd instead develop hardcoded
routines for the specific parity rows.
But for our pedagogical purposes, this approach is okay.</p>
<p>Let's demonstrate these with a test, just like before.</p>
<pre><code class="language-rust">#[cfg(test)]
#[test]
fn test_systematic_encode_decode() {
    // construct our systematic matrix by transforming a vandermonde matrix
    let A = systematic_encoding_matrix(&amp;vandermonde_encoding_matrix(&amp;X));

    // The A matrix should start with a K x K identity
    assert_eq!(A.slice_rows(0..K), Matrix::identity(K));

    // The remaining N-K rows should be parity
    assert_eq!(A.slice_rows(K..N), Matrix::new(N-K, K, vec![
        GF::new(146), GF::new(30),  GF::new(141),
        GF::new(155), GF::new(137), GF::new(19),
    ]));

    // test: trivial
    encode_decode_all(&amp;A,
        &amp;[GF::new(0), GF::new(0), GF::new(0)],
        &amp;[GF::new(0), GF::new(0), GF::new(0), GF::new(0), GF::new(0)],
    );

    // test: ones
    encode_decode_all(&amp;A,
        &amp;[GF::new(1), GF::new(1), GF::new(1)],
        &amp;[GF::new(1), GF::new(1), GF::new(1), GF::new(1), GF::new(1)],
    );

    // test: pattern
    encode_decode_all(&amp;A,
        &amp;[GF::new(100), GF::new(150), GF::new(200)],
        &amp;[GF::new(100), GF::new(150), GF::new(200), GF::new(64), GF::new(57)],
    );

    // test: random
    encode_decode_all(&amp;A,
        &amp;[GF::new(216), GF::new(196), GF::new(171)],
        &amp;[GF::new(216), GF::new(196), GF::new(171), GF::new(31), GF::new(66)],
    );
}
</code></pre>
<h3>Linear Independence and Projection to K-dimensions</h3>
<p>As this article shows, viewing Reed-Solomon codes as linear algebra can be very powerful. This view allows us to bring the tools of linear algebra to bear on erasure coding.</p>
<p>One way to view a Vandermonde Matrix is as <code>K</code> linearly independent vectors in an <code>N</code> dimensional space. These vectors have the fascinating property that we can project them to <code>K</code> dimensions (by removing rows) and they remain linearly independent! In this way, we can view Reed-Solomon encoding as a simple linear-combination of basis vectors with these special properties!</p>
<p>This observation leads us to non-Vandermonde encoding matrices that have this same key property. We saw previously that we could construct Systematic Matrices. But, we could also use <a href="https://en.wikipedia.org/wiki/Cauchy_matrix">Cauchy matrices</a> and these can lead to an interesting class of codes.</p>
<p>We can also develop bespoke encoding matrices. If the size is small enough, it's tractable for a computer to verify that all square $K \times K$ sub-matrices are invertible.</p>
<h3>Computational Complexity</h3>
<p>We now have two equivalent encoding and decoding approaches to Reed-Solomon. Let's compare them in computational complexity.</p>
<p>First some definitions:</p>
<ul>
<li><code>m</code>: number of polynomial evaluations / number of rows in the encoding matrix</li>
<li><code>n</code>: number of polynomial coefficients / number of columns in the encoding matrix</li>
<li><code>p</code>: number of parity rows used to decode a systematic code</li>
</ul>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Runtime Complexity</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td>Encoding (Polynomial)</td>
<td><code>O(mn)</code></td>
<td>Each evaluation is <code>O(n)</code> and we perform <code>m</code> evaluations</td>
</tr>
<tr>
<td>Encoding (Linear Algebra)</td>
<td><code>O(mn)</code></td>
<td>A matrix-vector multiply is <code>O(mn)</code>, i.e. each inner-product is <code>O(n)</code> and we perform <code>m</code> of them</td>
</tr>
<tr>
<td>Encoding (Systematic)</td>
<td><code>O(pn)</code></td>
<td>Only the parity rows need to be computed</td>
</tr>
<tr>
<td>Decoding (Polynomial)</td>
<td><code>O(n^2)</code></td>
<td>The inner-loop of interpolation is <code>O(n)</code> and we interpolate <code>n</code> points</td>
</tr>
<tr>
<td>Decoding (Linear Algebra)</td>
<td><code>O(n^3)</code></td>
<td>An LU factorization is <code>O(n^3)</code> and an <code>LU</code> solve is <code>O(n^2)</code></td>
</tr>
<tr>
<td>Decoding (Systematic)</td>
<td><code>O(p^3 + np)</code></td>
<td>Solving a $p \times p$ matrix after eliminating known data</td>
</tr>
</tbody>
</table>
<p>The above table shows that, while encoding is essentially equivalent, decoding is not. This is a commonly known effect of problems described in linear-algebra form.
Linear algebra is a very general toolset and often specific problems have special structure that yield better algorithms.</p>
<p>Despite this disadvantage, reasoning from a linear algebra perspective can still be very useful, as discussed above.</p>
<p>Also, consider the case of decoding a systematic code using parity rows. In many cases, we may only need a few parity rows to recover the full data vector. In other words, <code>p</code> is small and <code>O(p^3 + np)</code> approaches <code>O(n)</code>, becoming an effectively linear decoding time!</p>
<p>But, if <code>n</code> is large, can we do better that <code>O(mn)</code> encoding and <code>O(n^2)</code> decoding? Yes, we actually can. But, we'll have to wait for next time!</p>
<h2>Conclusion</h2>
<p>Well it's now <a href="https://youtu.be/g3ENX3aHlqU?si=fy64nJOVD6xGGf1d&amp;t=34">time to say goodbye</a>.</p>
<p>Here, we've explored Reed-Solomon from the perspective of Linear Algebra. We barely even touched a polynomial in this article and came away with an implementation that passed the exact same tests as the polynomial version! The linear algebra variant is shockingly simple, but pays for the luxury in decoding performance.</p>
<p>However, the linear algebra view is very useful for reasoning and allows us to introduce a wider variety of possible encoding matrices.</p>
<p>Linear Algebra will also be very useful in the next article where we'll discuss a third view of Reed-Solomon that will help us achieve very fast encoding and decoding.</p>
<p>What's your frequency, Kenneth? (soon)</p>
  </div>
  <div id="footer">
    <hr>
    <p><a href="/">home</a></p>
  </div>
</body>
</html>
